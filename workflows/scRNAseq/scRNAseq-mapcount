#!/usr/bin/env python3

__version__ = "0.1"

__description__ = """
single cell RNA-seq mapping and counting workflow v{version} - MPI-IE workflow for scRNA-seq
Steffen Heyne, Dominic Gruen
October, 2016

usage example:
    scRNAseq-mapcount -i input-dir -o output-dir mm10
""".format(version=__version__)


import argparse
import os
import signal
import subprocess
import sys
import textwrap
import time
import shutil

sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))))+"/shared/")
import common_functions

def parse_args():
    """
    Parse arguments from the command line.
    """

    parser = argparse.ArgumentParser(
        prog=sys.argv[0],
        formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent(__description__)
    )


    ## positional/required
    parser.add_argument("genome", metavar="GENOME", help="genome acronym of target organism (supported: 'dm6', 'hs37d5', 'mm9', 'mm10')")
    ## optional
    parser.add_argument("-c", "--configfile", dest="configfile", help="configuration file (config.yaml)", default=None)
    parser.add_argument("-i", "--input-dir", dest="indir", help="input directory containing the FASTQ files, either paired-end OR single-end data (default: '.')", default=None)
    parser.add_argument("-o", "--output-dir", dest="outdir", help="output directory (default: '.')", default=None)
    parser.add_argument("-j", "--jobs", dest="max_jobs", metavar="INT", help="maximum number of concurrently submitted Slurm jobs / cores if workflow is run locally (default: '%(default)s')", type=int, default=8)
    parser.add_argument("--local", dest="local", action="store_true", default=False, help="run workflow locally (default: jobs are submitted to Slurm queue)")
    parser.add_argument("--snakemake_options", dest="snakemake_options", metavar="STR", type=str, help="Snakemake options to be passed directly to snakemake, e.g. use --snakemake_options='--dryrun --rerun-incomplete --unlock --forceall'. (default: '%(default)s')", default="")
    parser.add_argument("--downsample", dest="downsample", metavar="INT", help="downsample the given number of reads from the head of each FASTQ file", type=int, default=None)
    parser.add_argument("--barcode_pattern", dest="barcode_pattern", metavar="STR", help="Defines the cell barcode and UMI order and length at the 5' end of R1 (Cel-seq protocol). 'N' defines UMI/random positions, X defines fixed positions; Default 'NNNNNNXXXXXX'", type=str, default="NNNNNNXXXXXX")
    parser.add_argument("--transcripts_exclude", dest="transcripts_exclude", metavar="STR", help="Keywords from gencode annotation that get removed from list of all transcripts", type=str, default=None)
    parser.add_argument("--cell_barcode_file", dest="barcode_file", metavar="STR", help="2-column file with cell-index (1st col) and barcode (2nd col). Default will use CelSeq@MPI-IE file with 192 barcodes. (Default: None)", type=str, default=None) 
    parser.add_argument("--trim", dest="trim", choices=['cutadapt', 'trimgalore'], default=None, help="trim reads with Cutadapt or TrimGalore. Default: no Trimming!")
    parser.add_argument("--tempdir", dest="tempdir", type=str, help="used prefix path for temporary directory created via mktemp. Created temp dir gets exported as $TMPDIR and is removed at the end of this wrapper! (default: '/data/extended/')", default="/data/extended/")
    parser.add_argument("-v", "--verbose", dest="verbose", action="store_true", default=False, help="verbose output")

#    parser.add_argument("--fastqc", dest="fastqc", action="store_true", default=False, help="run FastQC read quality control")
#    parser.add_argument("--dedup", dest="dedup", action="store_true", default=False, help="retain only de-duplicated reads/read pairs (given single-/paired-end data), recommended for ChIP-seq data")
#    parser.add_argument("--properpairs", dest="properpairs", action="store_true", default=False, help="retain only reads mapping in proper pairs")
#    parser.add_argument("--mapq", dest="mapq", metavar="INT", help="retain only reads with at least the given mapping quality (default: '%(default)s')", type=int, default=0)
#    parser.add_argument("--bw-binsize", dest="bw_binsize", metavar="INT", help="bin size of output files in bigWig format (default: '%(default)s')", type=int, default=10)
#    parser.add_argument("--gcbias", dest="gcbias", action="store_true", default=False, help="run computeGCBias quality control (long runtime!)")



    args = parser.parse_args()

    ## Variable sanity checking
    ## None

    ## correction to variables
    try:
        args.indir = os.path.abspath(args.indir)
    except:
        args.indir = os.path.abspath(os.getcwd())

    try:
        args.outdir = os.path.abspath(args.outdir)
    except:
        args.outdir = os.path.abspath(os.getcwd())

    try:
        args.configfile = os.path.abspath(args.configfile)
    except:
        args.configfile = None

    try:
        args.downsample = int(args.downsample)
    except:
        args.downsample = None

    return args


def main():
    args = parse_args()

    ## Require shared snakemake workflow directory
    this_script_dir = os.path.dirname(os.path.realpath(__file__))

    main_dir_path = os.path.join(os.path.dirname(os.path.dirname(this_script_dir)))
    if not os.path.isdir(main_dir_path):
        print("ERROR: Shared snakemake workflow directory is NOT available at:", main_dir_path)
        exit(1)

    ## Output directory + log directory
    cluster_logs_dir = os.path.join(args.outdir, "cluster_logs")
    subprocess.call("[ -d {cluster_logs_dir} ] || mkdir -p {cluster_logs_dir}".format(cluster_logs_dir=cluster_logs_dir), shell=True)

    ## required config variables
    user_configs = "--config maindir={main_dir_path} indir={indir} outdir={outdir} genome={genome}".format(
                                main_dir_path = main_dir_path,
                                indir = args.indir,
                                outdir = args.outdir,
                                genome = args.genome).split()

    ## optional config variables
    if args.downsample:
        user_configs += ["downsample={}".format(args.downsample)]
    if args.barcode_pattern:
        user_configs += ["barcode_pattern={}".format(args.barcode_pattern)]
    if args.trim:
        user_configs += ["trim={}".format(args.trim)]
    if args.transcripts_exclude:
        user_configs += ["transcripts_exclude={}".format(args.transcripts_exclude)]
#    if args.mapq:
#        user_configs += ["mapq={}".format(args.mapq)]
#    if args.bw_binsize:
#        user_configs += ["bw_binsize={}".format(args.bw_binsize)]
#    if args.gcbias:
#        user_configs += ["gcbias=True"]
    if args.verbose:
        user_configs += ["verbose=True"]
	
    if args.barcode_file:
        if os.path.exists(os.path.abspath(args.barcode_file)):
            user_configs += ["barcode_file={}".format(args.barcode_file)]
        else:
            print("\nBarcode file not found! {}\n".format(args.barcode_file))
            exit(1)
	## MPI
    snakemake_module_load = "module load snakemake slurm &&".split()
    ## laptop
    #snakemake_module_load = " ".split()
    snakemake_cmd = """
                    snakemake {snakemake_options} --latency-wait 30 --snakefile {snakefile} --jobs {max_jobs} {user_configs} --directory {outdir}
                    """.format( snakefile = os.path.join(this_script_dir, "Snakefile"),
                                max_jobs = args.max_jobs,
                                user_configs = " ".join(user_configs),
                                outdir = args.outdir,
                                cluster_logs_dir = os.path.abspath(cluster_logs_dir),
                                snakemake_options=args.snakemake_options,
                              ).split()

    if args.configfile:
        snakemake_cmd += ["--configfile", args.configfile]

    if args.verbose:
        snakemake_cmd.append("--printshellcmds")

    if not args.local:
        snakemake_cmd += ["--cluster 'SlurmEasy --threads {threads} --log", cluster_logs_dir, "--name {rule}.snakemake'"]

    snakemake_log = "2>&1 | tee -a {}/scRNAseq-mapcount.log".format(args.outdir).split()

    ## create local temp dir and add this path to environment as $TMPDIR variable 
    ## on SLURM: $TMPDIR is set, created and removed by SlurmEasy on cluster node 
    temp_path = common_functions.make_temp_dir(args.tempdir, args.outdir)
    snakemake_exports = ("export TMPDIR="+temp_path+" && ").split()

    cmd = " ".join(snakemake_exports + snakemake_module_load + snakemake_cmd + snakemake_log)

    if args.verbose:
        print("\n", cmd, "\n")

    ## Write snakemake_cmd to log file
    with open(os.path.join(args.outdir,"scRNAseq-mapcount.log"),"w") as f:
        f.write(" ".join(sys.argv)+"\n\n")
        f.write(cmd+"\n\n")

    ## Run snakemake
    p = subprocess.Popen(cmd, shell=True)
    if args.verbose:
        print("PID:", p.pid, "\n")
    try:
        p.wait()
    except:
        print("\nWARNING: Snakemake terminated!!!")
        if p.returncode != 0:
            if p.returncode:
                print("Returncode:", p.returncode)

            # kill snakemake and child processes
            subprocess.call(["pkill", "-SIGTERM", "-P", str(p.pid)])
            print("SIGTERM sent to PID:", p.pid)

            # # kill grid engine jobs
            # time.sleep(10)
            # job_ids = subprocess.check_output("""ls {cluster_logs_dir} | awk -F "." '{{print $NF}}' | sed 's/e\|o//' | sort -u""".format(cluster_logs_dir=cluster_logs_dir), shell=True).split()
            # for job_id in job_ids:
            #     subprocess.call( "qdel {} 2>&1 >/dev/null".format(str(job_id)), shell="bash" )
    
    ## remove temp dir
    if (temp_path != "" and os.path.exists(temp_path)):
        shutil.rmtree(temp_path, ignore_errors=True)
        if args.verbose: 
            print("Temp directory removed ("+temp_path+")!\n")
            
if __name__ == "__main__":
    #print "Args:", sys.argv
    main()
